{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The csv file does not have a column header so to make sure\n",
    "# the first row of csv file isn't treated as a header, we put \n",
    "# header = None while reading the csv file\n",
    "\n",
    "# Also I am giving names of cloumns using names parameter ( the numbers are column names we won't be needing)\n",
    "\n",
    "# enter the location of downloaded file in read csv\n",
    "\n",
    "tweets = pd.read_csv('/Users/rishabh/Downloads/training.1600000.processed.noemoticon.csv', encoding='latin-1',header = None, names = ['label',1,2,3,4,'comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label           1                             2         3                4  \\\n",
       "0      0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                            comments  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at the data\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the tweets contains stop words ( example is, are, the ) and punctuations\n",
    "# the below function removes these from the comments\n",
    "\n",
    "def tweet_to_words(text):\n",
    "    \n",
    "    # string library consists of punctuation consisting of most of the punctuation characters\n",
    "    punctuation = list(string.punctuation)\n",
    "    # tokenizer separates sentences into words\n",
    "    words = word_tokenize(text.lower())\n",
    "    # stopwords library consists of the stop words of english ( by specifying english )\n",
    "    stops = list(stopwords.words(\"english\"))\n",
    "    # combining list of stopwords and punctuations\n",
    "    stops = stops + punctuation\n",
    "    # select only those words which are not in stops list\n",
    "    good_words = [w for w in words if not w in stops]\n",
    "    return \" \".join(good_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply to the comments section of csv file\n",
    "# Depending upon the machine/ instance you are working upon, cleaning of data will take time\n",
    "# ( 30 - 40 mins in modern hardware, 5 mins using a gpu approximately ) since data set has 1.6 million rows approximately\n",
    "\n",
    "data = tweets['comments'].apply(tweet_to_words)\n",
    "target = tweets['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data ( training and testing )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, target, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectorizer converts all features ( vocabulary size ) into a matrix with each row describing if the word occurs\n",
    "# and count of it\n",
    "# max_features = integer x determines the maximum number of features in training set to be taken for training the model\n",
    "# instead of the whole vocabulary size\n",
    "# ngram_range tells if only single words are to be taken as features or combination of words can also be taken\n",
    "# combination is beneficial since for example - what a good devil you are. 'good' might seem as if comment might be positive\n",
    "# but 'good devil' may just make the whole comment negative\n",
    "v = CountVectorizer(max_features = 20000, ngram_range=(1,2))\n",
    "train_features = v.fit_transform(x_train)\n",
    "# i am going to store vectorizer's object since next time we won't have to go through creating entire dataset for training and testing\n",
    "# i am using something called pickle\n",
    "\n",
    "pickle.dump(v,open(\"vectorizerpickle.pickle\",\"wb\"))\n",
    "\n",
    "# now the load of object can be used anytime even if the above steps are not performed\n",
    "\n",
    "vectorizer = pickle.load(open(\"vectorizerpickle.pickle\", \"rb\"))\n",
    "\n",
    "test_features = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76673\n"
     ]
    }
   ],
   "source": [
    "# We will use Multinomial naive bayes as our model for classification\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_features, y_train)\n",
    "\n",
    "# cleaning and training takes a lot of time. So I will save this created model using pickle\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(test_features, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      "[[156527  43207]\n",
      " [ 50101 150165]]\n",
      "Classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.78      0.77    199734\n",
      "          4       0.78      0.75      0.76    200266\n",
      "\n",
      "avg / total       0.77      0.77      0.77    400000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "Y_pred = clf.predict(test_features)\n",
    "print(\"Confusion matrix : \")\n",
    "print(confusion_matrix(y_test, Y_pred))\n",
    "print(\"Classification report : \")\n",
    "print(classification_report(y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction function for a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter comment : it's gonna be fine. this world is full of negativity but i hope you see the good part as well. i am with you\n",
      "Comment posted : it's gonna be fine. this world is full of negativity but i hope you see the good part as well. i am with you\n"
     ]
    }
   ],
   "source": [
    "# defining function for prediction\n",
    "\n",
    "def predict(s):\n",
    "    sentence = vectorizer.transform([s])\n",
    "    if(loaded_model.predict(sentence)[0]==0):\n",
    "        print(\"Your comment might be negative/abusive. It may hurt someone. Are you sure you want to post it ?(y/n) : \")\n",
    "        k = input()\n",
    "        if(k == 'y' or k=='Y'):\n",
    "            print(\"Comment posted : \"+s)\n",
    "        else:\n",
    "            print(\"Thank you !\")\n",
    "    else:\n",
    "        print(\"Comment posted : \"+s)\n",
    "\n",
    "s = input('Enter comment : ')\n",
    "predict(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
